{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create User Profile \n",
    "This script is to crawl specific websites to create a profile for our test user.  \n",
    "We will use beautiful soup to parse some websites & store their texts as json.  \n",
    "We are careful to include specific fields\n",
    "* title\n",
    "* url\n",
    "* html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = [\n",
    "    'https://towardsdatascience.com/playing-connect-4-with-deep-q-learning-76271ed663ca', \n",
    "    'https://towardsdatascience.com/teach-your-ai-how-to-walk-5ad55fce8bca', \n",
    "    'https://www.freecodecamp.org/news/an-introduction-to-deep-q-learning-lets-play-doom-54d02d8017d8/', \n",
    "    'https://towardsdatascience.com/reinforcement-learning-demystified-markov-decision-processes-part-1-bf00dda41690'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "page = requests.get(links[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(page.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Playing Connect 4 with Deep Q-Learning | by Lee Schmalz | Towards Data ScienceGet startedOpen in appSign inGet startedFollow579K Followers·Editors' PicksFeaturesDeep DivesGrowContributeAboutGet startedOpen in appPlaying Connect 4 with Deep Q-LearningExploring the power of Reinforcement Learning through a well-known game environmentLee SchmalzJun 24, 2020·11 min readCredit: allthefreestock.comDeep Q-Learning may be one of the most important algorithms in all of Reinforcement Learning as it lacks limitation on the observations it can make and the actions it can take within complex environments. This method of Reinforcement Learning incorporates deep neural networks in a way that allows an agent to ‘play’ an environment repeatedly and learn the environment over time through a system of observations, actions, and rewards. This structure has obvious benefits over a standard deep neural network implementation as it allows the agent to interact with its surroundings, receive feedback from its surroundings, and then optimize for desirable (highly rewarded) future actions. In this article, we will be looking at a familiar environment that was recently instantiated by Kaggle in one of their Kaggle competitions (https://www.kaggle.com/c/connectx).Example board state from Kaggle environmentBefore we start exploring the structure of a Deep Q-Learning agent to play Connect 4, let’s first briefly overview the structure of a simple, much less useful Q-Learning agent. The basic idea of Q-Learning is to create a map of the entire observation space, and within this map, record the agents actions. Subsequently, every time the agent encounters the same observation, it will incrementally update the action it previously took based on whether its previous action earned a positive or negative reward. This data structure in which previous actions for each observation in the observation space are stored is known as a Q-Table. This incremental updating of the Q-Table is most commonly done by the following Q-Learning equation:While this equation appears complicated upon first glance, it is simply using the expected reward to update the specific location of the Q-Table that corresponds to the current observation. Understanding of this process is much better visualized through a code example, though not necessary to work through our Deep Q-Learning project, since we will instead be using a Deep Neural Network to update our “Q-Table” (more on this later).To better understand the nature of Q-learning, let’s frame an example observation from the perspective of our very well-known environment, Connect 4. The following is the above example state of a Connect 4 board during a game, in other words an element of the observation space (Player 1 chips: 1, Player 2 chips: 2, empty spaces: 0)Previously shown board observation as seen by the neural networkHere we see a specific arrangement of previously played pieces that would be sent to the agent as an observation. The agent would take this observation and find this observation within its Q-Table. Essentially asking itself the question, “What did I do last time I saw this arrangement of pieces and how can I do better this time?”. It would then proceed to act in the proper manner and throughout the process updating the action in the proper location in the Q-Table for each move. From a broad overview, this seems like a great use case for Q-Learning… other than one subtle exception. Did you catch it?The Limitations of standard Q-LearningThe Q-Learning structure is very useful for some environments, but the number of environments in which it is functional is very limited. This is due to the previously stated phrase: “The basic idea of Q-Learning, is to create a map of the entire observation space…”. Think about what this means even in the context of our simple Connect 4 environment. In Connect 4, we have 42 entries that can be filled by any of a player 1 chip, a player 2 chip, or no chip. We must also give gravity its due; in that a chip can only be played in a location of the grid if there is a previously played chip in the space underneath it. Further, if any 4 chips of the same color are lined up, the game is over and thus future situations cannot be included in the observation space. This amounts to be a somewhat complex calculation as we’ve laid out; nonetheless, you can see how the number of possible observations starts to explode very quickly. According to the online Encyclopedia of Integer Sequences, the result is 4,531,985,219,092 (oeis.org/A212693). I don’t know about you, but it seems a 4 quadrillion entry Q-Table is just a bit too large for my 1TB hard drive. So, standard Q-Learning is not a good place to start, let’s turn to a different solution.Deep Q-LearningWith the foundational structure of Q-Learning in mind, Deep Q-Learning is very easily understood; the only difference being a substitution of the Q-Table. The limitation of the Q-Table for environments with large observation spaces is reached very quickly in Q-Learning. If only we knew of some sort of algorithm that could take an observation along with some sort loss function that we calculate from the reward, and make some sort of generalization about the observed environment in order to choose a single output (or action), so that we don’t have to store so many environment observations. As anyone with an interest in machine learning knows, Deep Neural Networks, are a tool aimed at just this issue. In summary, to simplify the staggering size of our observation space in the game of Connect 4, we will use a standard Q-Learning structure with a simple substitution of a Deep Neural Network for what would have otherwise been a Q-Table reference.Connect 4 Kaggle EnvironmentFor convenience, we will be using the Connect X framework from an ongoing Kaggle competition (with a few modifications) to build our agent. This will allow us to very simply get observations from and send actions to the environment without having to build the game of Connect 4 ourselves. The following 4 lines of code deals with the construction of the environment along with a simulation of two agents playing Connect 4 against each other, in this case two random agents (https://pypi.org/project/kaggle-environments/). This environment supplies both a ‘random’ agent and a ‘negamax’ agent for training; in which, the random agent simply makes random legal moves, and the negamax agent makes beginner level, but better than random legal moves. With this in mind, we realize the concept of self-play will be essential for training in which our model plays against itself.Environment Reward ModificationThe only slight change made to environment provided by Kaggle was manual calculation of when the game is over. This allowed for reward determination for different kinds of wins, losses, and invalid moves. For example, when our agent is training against a random agent, vertical victories are very easy to come by since the random agent will only block the vertical victory 3/7 of the time. In this specific training instance it might be useful to place less reward on vertical victories that cause the agent to only play in column 3, for example, and still win 4 out of every 7 games using an obviously bad strategy. The win/loss determination is demonstrated by the following function. This is simply brute force iteration through all possible vertical, horizontal, and diagonal configurations of the board, respectively.Creating the Neural Network model and OptimizerThese steps are somewhat self-explanatory with the assumption in mind that the reader has had an exposure to neural networks before; but, a brief overview of each of the creation of the model, the loss function, and the optimization follows. First, the model is simply a dense neural network built from the very convenient use of the tensorflow.keras.layers module. I first tried to use a convolutional neural network as I assumed relational information would be beneficial, but found this not to perform as well (I assume due to the small size of the board). The output of the network has 7 neurons, each corresponding to the action of dropping a chip into one of 7 different columns of the board.A similar structure, where our hidden layers have 50 neurons each. Created using alexlenail.me web toolThe loss function simply takes the sum of the rewards of an episode (or a complete game of Connect 4), applies them to each input and output of the network (observation and action, respectively), performs cross entropy and gives each observation/action pair a loss value that is inversely proportional to the reward received. For example, if the agent wins a game, all observation/action pairs throughout the game will receive a reward of 20, the highest reward offered in our reward structure. This will in turn result in a small loss value, hence the inverse relationship. This inverse relationship is so that our optimizer optimizes the network towards positive rewards, as the goal of any standard optimizer is to minimize the loss of the network. In theory (and much less often in practice), minimizing loss will then increase reward and in turn increase the ability of the network to make decisions that result in wins in the game of Connect 4.The optimizing step here in the train_step() function may look a bit new to even someone experienced in deep learning. This is because it uses the GradientTape() module which is new to Tensorflow as of version 2.0. This module in summary ‘watches’ (or records) the gradients of the network as they are adjusted and allows a network to be trained using custom loss functions with ease, as we have implemented here, a big improvement from the model.fit() method of older versions.Choosing an Action with Epsilon Decay — Exploration vs. ExploitationIn all Reinforcement Learning problems, there exists an unavoidable trade-off between exploration and exploitation. Exploration is where an agent makes some out of character decision (not dictated by the network) to try to find new strategies and potentially be rewarded from them; thus, an opportunity to learn better strategies. Exploitation is to exploit the strategies the network has already learned. For example, if an agent has learned a bit how to play Connect 4 and it always exploits the techniques it has already learned, it will never be presented with new observation/action pairs to learn from. On the other hand, if the agent is always exploring new strategies, it is essentially always playing random moves and never reaching a high level of game play to learn from. The way this problem is dealt with typically is through a phenomenon known as epsilon decay. The epsilon in epsilon decay can be thought of as the probability the agent will choose a random action rather than an action as chosen by then network. The decay in epsilon decay is just that: since towards the beginning of the training cycle we expect the agent to be dumb, for lack of a better term, we will let it make largely random decisions, thus letting the agent explore more than exploit, since it hasn’t yet learned anything to exploit. Over time, the epsilon will get smaller and smaller, that is: the agent will start exploiting its learned knowledge more and more; and will explore random actions less and less. This agent will use the epsilon decay function epsilon = .99985^x where x is the number of episodes trained. This would be adjusted accordingly to the number of episodes of the training period, in this case we use 40,000.Created with Desmos.com web toolCreating the AgentThis part is very simple. The agent is simply a function that takes in an observation, and spits out an action. Shown below is a very simple agent in which it sends the observation through the neural network and outputs an action. Though there is a small addition, a bug fix if you will: in the ‘else’ statement you’ll see it simply chooses the next highest probability action predicted by the network in the case that the action is determined to be invalid. An action is invalid if the network decides to play in a column that is already full. You’ll notice though, after substantial training and proper reward structures, the agent eventually learns to play only valid actions; making this sort of a ‘just in case’ feature.Finally, Time to Start TrainingThe work flow here is relatively straight forward now that all of the structural functions are built. We start by instantiating our memory object that will be used to pass information to the network from the environment. We then iterate through each episode, 40,000 in this case, properly passing observations and rewards to the network as dictated by the previously described functions. Notice our custom specified rewards, as previously mentioned. The training process consisted of three training sessions in total: the first training session against the random agent, the second session against the negamax agent, and the third session playing against itself (or the same model trained to play player 2’s role, more specifically).The ResultThe following is a game played by our agent (blue) against the negamax agent (grey). Overall, I am impressed with the agent’s performance; it is obvious the agent has figured out how to block wins, how to win itself, and also how to play only in the columns that are not already full (these would’ve been corrected anyway, but it attempted no invalid moves at this point in training). The move I am most impressed by in this example is its second to last move, move 32. This move is impressive because it not only sets up a win, but it actually guarantees a win by setting up both a horizontal win and a diagonal win at the same time.ConclusionA few main takeaways from this project to address:I am aware that Deep Q-Learning is likely not the best way to go about solving this environment. I think if I was actually competing in the Kaggle competition with this agent, a deep learning assisted Monte Carlo Tree Search algorithm is the best way to go about this. The reason I chose this route is that I am more interested in a topic like Deep Q-Learning over tree search based algorithms due to the complexity of the environment each are able to handle. Deep Q-Learning can handle any observation space so long as the observations themselves are not too data intensive for the network. I feel this sort of algorithm is more useful to experiment with as it is more likely to be implemented in real world environments, whereas a tree search type algorithm is specifically designed for constrained environments, such as board games.Though the agent is certainly impressive in its actions; I don’t think it would compete with human level intelligence. The training process of this algorithm took just over a day and I didn’t want to keep allocating the GPU resources to this to let it keep training, but I would be interested in seeing if excessive training of this algorithm would allow even greater improvement, or if it was near its skill level maximum threshold.Lee SchmalzMathematics and Molecular Biology Graduate pursuing ideas in Data Science.Follow93 2 Sign up for The VariableBy Towards Data ScienceEvery Thursday, the Variable delivers the very best of Towards Data Science: from hands-on tutorials and cutting-edge research to original features you don't want to miss.\\xa0Take a look.Get this newsletterBy signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices.Check your inboxMedium sent you an email at  to complete your subscription.93\\xa093\\xa02 Deep LearningArtificial IntelligenceReinforcement LearningQ LearningPythonMore from Towards Data ScienceFollowYour home for data science. A Medium publication sharing concepts, ideas and codes.Read more from Towards Data ScienceMore From MediumA Complete Yet Simple Guide to Move From Excel to PythonFrank Andrade in Towards Data ScienceHow to Create Mathematical Animations like 3Blue1Brown Using PythonKhuyen Tran in Towards Data ScienceHidden Gems of PythonCharudatta Manwatkar in Towards Data Science5 Reasons Why I Left the AI IndustryAlberto Romero in Towards Data ScienceA checklist to track your Data Science progressPascal Janetzky in Towards Data Science6 Data Science Slack Workspaces You Need to JoinSara A. Metwalli in Towards Data ScienceA Beginner’s Guide to Regression Analysis in Machine LearningAqeel Anwar in Towards Data ScienceKiller  Data Processing Tricks For Python ProgrammersEmmett Boudreau in Towards Data ScienceAboutHelpLegalGet the Medium app\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.getText()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_links(links):\n",
    "    texts = []\n",
    "    for link in links: \n",
    "        page = requests.get(link)\n",
    "        soup = BeautifulSoup(page.content, 'html.parser')\n",
    "        texts.append(soup.getText())\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = parse_links(links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate large user profile "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_profile(content_list): \n",
    "    # Easy method: concatenation \n",
    "    return ' '.join(content_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_profile = generate_profile(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load in TFIDF Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kejio/opt/anaconda3/lib/python3.8/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import TfidfModel,LdaModel\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim import corpora, similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_model = TfidfModel.load('./models/tfidf-sample10000')\n",
    "dictionary = corpora.Dictionary.load('./models/dictionary/sample10000Dict')\n",
    "# Load corpus\n",
    "loaded_corpus = corpora.MmCorpus('./models/sample10000corpus.mm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess User Profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_text = simple_preprocess(user_profile)\n",
    "user_tfidf_vec = tfidf_model[dictionary.doc2bow(user_text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This should be done at start -> Can take a while\n",
    "index = similarities.Similarity(None, tfidf_model[loaded_corpus], len(dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This also takes a second. The consine similarity is done here\n",
    "res = index[user_tfidf_vec]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(5321, 0.31066835), (555, 0.29828316), (2193, 0.29372314), (1981, 0.28961307), (3182, 0.25882074), (9372, 0.23132288), (5581, 0.22526635), (9142, 0.22428486), (5046, 0.22016639), (4975, 0.21627207)]\n"
     ]
    }
   ],
   "source": [
    "# Print top 10 documents \n",
    "print(list(sorted(enumerate(res), key=lambda x: x[1], reverse=True))[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_corpus.num_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5321 0.31066835\n",
      "555 0.29828316\n",
      "2193 0.29372314\n",
      "1981 0.28961307\n",
      "3182 0.25882074\n",
      "9372 0.23132288\n",
      "5581 0.22526635\n",
      "9142 0.22428486\n",
      "5046 0.22016639\n",
      "4975 0.21627207\n"
     ]
    }
   ],
   "source": [
    "for article_idx, score in sorted(enumerate(res), key=lambda x: -x[1])[:10]: \n",
    "    print(article_idx, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n(n, index, user_tfidf_vec): \n",
    "    \"\"\"Gets the index of the top n documents similar to the user profile\"\"\"\n",
    "    res = index[user_tfidf_vec]\n",
    "    top_ids = []\n",
    "    top_scores = []\n",
    "    for article_idx, score in sorted(enumerate(res), key=lambda x: -x[1])[:n]: \n",
    "        top_ids.append(article_idx)\n",
    "        top_scores.append(score)\n",
    "    return top_ids, top_scores\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_ids, top_scores = get_top_n(5, index, user_tfidf_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5321, 555, 2193, 1981, 3182]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the Top 5 Docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import ijson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in Dataset \n",
    "class MyCorpusJSON: \n",
    "    def __init__(self, json_link, column): \n",
    "        # idx is the index of the row where the text content is\n",
    "        self.json_link = json_link \n",
    "        self.text_column = column\n",
    "        self.count = 0\n",
    "    \n",
    "    def __len__(self): \n",
    "        return self.count\n",
    "    \n",
    "    def get_nth(self, n): \n",
    "        return next(itertools.islice(self.generator(), n, None))\n",
    "    \n",
    "    def generator(self): \n",
    "        with open(self.json_link) as json_file: \n",
    "            parser = ijson.items(json_file, 'item')\n",
    "            for obj in parser:\n",
    "                yield obj\n",
    "                \n",
    "    def __iter__(self):  \n",
    "        with open(self.json_link) as json_file: \n",
    "            parser = ijson.items(json_file, 'item')\n",
    "            for obj in parser:\n",
    "                self.count += 1\n",
    "                yield obj[self.text_column]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "link = './data/samples/sample10000.json'\n",
    "text_column = 'html_text'\n",
    "mycorpus = MyCorpusJSON(link, text_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'url': 'https://medium.com/machine-learning-for-humans/reinforcement-learning-6eacf258b265',\n",
       " 'title': 'Machine Learning for Humans, Part 5: Reinforcement Learning',\n",
       " 'author': {'name': None,\n",
       "  'url': 'https://medium.com/@v_maini',\n",
       "  'twitter': '@v_maini'},\n",
       " 'image_url': 'https://cdn-images-1.medium.com/max/1200/1*jne9wcY21o_e_ztLyQSPSw.png',\n",
       " 'html_text': 'Vishal MainiResearch comms @DeepMindAI. Previously @Upstart, @Yale, @TrueVenturesTEC.Aug 19, 2017Machine Learning for Humans, Part 5: Reinforcement LearningExploration and exploitation. Markov decision processes. Q-learning, policy learning, and deep reinforcement learning.[Update 9/1/17] This series is now available as a full-length e-book! Download here.“I just ate some chocolate for finishing the last section.”In supervised learning, training data comes with an answer key from some godlike “supervisor”. If only life worked that way!In reinforcement learning (RL) there’s no answer key, but your reinforcement learning agent still has to decide how to act to perform its task. In the absence of existing training data, the agent learns from experience. It collects the training examples (“this action was good, that action was bad”) through trial-and-error as it attempts its task, with the goal of maximizing long-term reward.In this final section of Machine Learning for Humans, we will explore:The exploration/exploitation tradeoffMarkov Decision Processes (MDPs), the classic setting for RL tasksQ-learning, policy learning, and deep reinforcement learningand lastly, the value learning problemAt the end, as always, we’ve compiled some favorite resources for further exploration.Let’s put a robot mouse in a\\xa0mazeThe easiest context in which to think about reinforcement learning is in games with a clear objective and a point system.Say we’re playing a game where our mouse 🐭 is seeking the ultimate reward of cheese at the end of the maze (🧀 +1000 points), or the lesser reward of water along the way (💧+10 points). Meanwhile, robo-mouse wants to avoid locations that deliver an electric shock (⚡ -100 points).The reward is\\xa0cheese.After a bit of exploration, the mouse might find the mini-paradise of three water sources clustered near the entrance and spend all its time exploiting that discovery by continually racking up the small rewards of these water sources and never going further into the maze to pursue the larger prize.But as you can see, the mouse would then miss out on an even better oasis further in the maze, or the ultimate reward of cheese at the end!This brings up the exploration/exploitation tradeoff. One simple strategy for exploration would be for the mouse to take the best known action most of the time (say, 80% of the time), but occasionally explore a new, randomly selected direction even though it might be walking away from known reward.This strategy is called the epsilon-greedy strategy, where epsilon is the percent of the time that the agent takes a randomly selected action rather than taking the action that is most likely to maximize reward given what it knows so far (in this case, 20%). We usually start with a lot of exploration (i.e. a higher value for epsilon). Over time, as the mouse learns more about the maze and which actions yield the most long-term reward, it would make sense to steadily reduce epsilon to 10% or even lower as it settles into exploiting what it knows.It’s important to keep in mind that the reward is not always immediate: in the robot-mouse example, there might be a long stretch of the maze you have to walk through and several decision points before you reach the cheese.The agent observes the environment, takes an action to interact with the environment, and receives positive or negative reward. Diagram from Berkeley’s CS 294: Deep Reinforcement Learning by John Schulman & Pieter\\xa0AbbeelMarkov Decision Processes (MDPs)The mouse’s wandering through the maze can be formalized as a Markov Decision Process, which is a process that has specified transition probabilities from state to state. We will explain it by referring to our robot-mouse example. MDPs include:A finite set of states. These are the possible positions of our mouse within the maze.A set of actions available in each state. This is {forward, back} in a corridor and {forward, back, left, right} at a crossroads.Transitions between states. For example, if you go left at a crossroads you end up in a new position. These can be a set of probabilities that link to more than one possible state (e.g. when you use an attack in a game of Pokémon you can either miss, inflict some damage, or inflict enough damage to knock out your opponent).Rewards associated with each transition. In the robot-mouse example, most of the rewards are 0, but they’re positive if you reach a point that has water or cheese and negative if you reach a point that has an electric shock.A discount factor γ between 0 and 1. This quantifies the difference in importance between immediate rewards and future rewards. For example, if γ is\\xa0.9, and there’s a reward of 5 after 3 steps, the present value of that reward is\\xa0.9³*5.Memorylessness. Once the current state is known, the history of the mouse’s travels through the maze can be erased because the current Markov state contains all useful information from the history. In other words, “the future is independent of the past given the present”.Now that we know what an MDP is, we can formalize the mouse’s objective. We’re trying to maximize the sum of rewards in the long term:Let’s look at this sum term by term. First of all, we’re summing across all time steps t. Let’s set γ at 1 for now and forget about it. r(x,a) is a reward function. For state x and action a (i.e., go left at a crossroads) it gives you the reward associated with taking that action a at state x. Going back to our equation, we’re trying to maximize the sum of future rewards by taking the best action in each state.Now that we’ve set up our reinforcement learning problem and formalized the goal, let’s explore some possible solutions.Q-learning: learning the action-value functionQ-learning is a technique that evaluates which action to take based on an action-value function that determines the value of being in a certain state and taking a certain action at that state.We have a function Q that takes as an input one state and one action and returns the expected reward of that action (and all subsequent actions) at that state. Before we explore the environment, Q gives the same (arbitrary) fixed value. But then, as we explore the environment more, Q gives us a better and better approximation of the value of an action a at a state s. We update our function Q as we go.This equation from the Wikipedia page on Q-learning explains it all very nicely. It shows how we update the value of Q based on the reward we get from our environment:Let’s ignore the discount factor γ by setting it to 1 again. First, keep in mind that Q is supposed to show you the full sum of rewards from choosing action Q and all the optimal actions afterward.Now let’s go through the equation from left to right. When we take action at in state st, we update our value of Q(st,at) by adding a term to it. This term contains:Learning rate alpha: this is how aggressive we want to be when updating our value. When alpha is close to 0, we’re not updating very aggressively. When alpha is close to 1, we’re simply replacing the old value with the updated value.The reward is the reward we got by taking action at at state st. So we’re adding this reward to our old estimate.We’re also adding the estimated future reward, which is the maximum achievable reward Q for all available actions at xt+1.Finally, we subtract the old value of Q to make sure that we’re only incrementing or decrementing by the difference in the estimate (multiplied by alpha of course).Now that we have a value estimate for each state-action pair, we can select which action to take according to our action-selection strategy (we don’t necessarily just choose the action that leads to the most expected reward every time, e.g. with an epsilon-greedy exploration strategy we’d take a random action some percentage of the time).In the robot mouse example, we can use Q-learning to figure out the value of each position in the maze and the value of the actions {forward, backward, left, right} at each position. Then we can use our action-selection strategy to choose what the mouse actually does at each time step.Policy learning: a map from state to\\xa0actionIn the Q-learning approach, we learned a value function that estimated the value of each state-action pair.Policy learning is a more straightforward alternative in which we learn a policy function, π, which is a direct map from each state to the best corresponding action at that state. Think of it as a behavioral policy: “when I observe state s, the best thing to do is take action a”. For example, an autonomous vehicle’s policy might effectively include something like: “if I see a yellow light and I am more than 100 feet from the intersection, I should brake. Otherwise, keep moving forward.”A policy is a map from state to\\xa0action.So we’re learning a function that will maximize expected reward. What do we know that’s really good at learning complex functions? Deep neural networks!Andrej Karpathy’s Pong from Pixels provides an excellent walkthrough on using deep reinforcement learning to learn a policy for the Atari game Pong that takes raw pixels from the game as the input (state) and outputs a probability of moving the paddle up or down (action).In a policy gradient network, the agent learns the optimal policy by adjusting its weights through gradient descent based on reward signals from the environment. Image via http://karpathy.github.io/2016/05/31/rl/If you want to get your hands dirty with deep RL, work through Andrej’s post. You will implement a 2-layer policy network in 130 lines of code, and will also learn how to plug into OpenAI’s Gym, which allows you to quickly get up and running with your first reinforcement learning algorithm, test it on a variety of games, and see how its performance compares to other submissions.DQNs, A3C, and advancements in deep\\xa0RLIn 2015, DeepMind used a method called deep Q-networks (DQN), an approach that approximates Q-functions using deep neural networks, to beat human benchmarks across many Atari games:We demonstrate that the deep Q-network agent, receiving only the pixels and the game score as inputs, was able to surpass the performance of all previous algorithms and achieve a level comparable to that of a professional human games tester across a set of 49 games, using the same algorithm, network architecture and hyperparameters. This work bridges the divide between high-dimensional sensory inputs and actions, resulting in the first artificial agent that is capable of learning to excel at a diverse array of challenging tasks. (Silver et al., 2015)Here is a snapshot of where DQN agents stand relative to linear learners and humans in various domains:These are normalized with respect to professional human games testers: 0% = random play, 100% = human performance. Source: DeepMind’s DQN paper, Human-level control through deep reinforcement learningTo help you build some intuition for how advancements are made in RL research, here are some examples of improvements on attempts at non-linear Q-function approximators that can improve performance and stability:Experience replay, which learns by randomizing over a longer sequence of previous observations and corresponding reward to avoid overfitting to recent experiences. This idea is inspired by biological brains: rats traversing mazes, for example, “replay” patterns of neural activity during sleep in order to optimize future behavior in the maze.Recurrent neural networks (RNNs) augmenting DQNs. When an agent can only see its immediate surroundings (e.g. robot-mouse only seeing a certain segment of the maze vs. a birds-eye view of the whole maze), the agent needs to remember the bigger picture so it remembers where things are. This is similar to how humans babies develop object permanence to know things exist even if they leave the baby’s visual field. RNNs are “recurrent”, i.e. they allow information to persist on a longer-term basis. Here’s an impressive video of a deep recurrent Q-network (DQRN) playing Doom.Paper: https://arxiv.org/abs/1609.05521. Source: Arthur Juliani’s Simple Reinforcement Learning with Tensorflow seriesIn 2016, just one year after the DQN paper, DeepMind revealed another algorithm called Asynchronous Advantage Actor-Critic (A3C) that surpassed state-of-the-art performance on Atari games after training for half as long (Mnih et al., 2016). A3C is an actor-critic algorithm that combines the best of both approaches we explored earlier: it uses an actor (a policy network that decides how to act) AND a critic (a Q-network that decides how valuable things are). Arthur Juliani has a nice writeup on how A3C works specifically. A3C is now OpenAI’s Universe Starter Agent.Since then, there have been countless fascinating breakthroughs\\u200a—\\u200afrom AIs inventing their own language to teaching themselves to walk in a variety of terrains. This series only scratches the surface on the cutting edge of RL, but hopefully it will serve as a starting point for further exploration!As a parting note, we’d like to share this incredible video of DeepMind’s agents that learned to walk… with added sound. Grab some popcorn, turn up the volume, and witness the full glory of artificial intelligence.😱😱😱Practice materials & further\\xa0readingCodeAndrej Karpathy’s Pong from Pixels will get you up-and-running quickly with your first reinforcement learning agent. As the article describes, “we’ll learn to play an ATARI game (Pong!) with PG, from scratch, from pixels, with a deep neural network, and the whole thing is 130 lines of Python only using numpy as a dependency (Gist link).”Next, we’d highly recommend Arthur Juliani’s Simple Reinforcement Learning with Tensorflow tutorial. It walks through DQNs, policy learning, actor-critic methods, and strategies for exploration with implementations using TensorFlow. Try understanding and then re-implementing the methods covered.Reading +\\xa0lecturesRichard Sutton’s book, Reinforcement Learning: An Introduction\\u200a—\\u200aa fantastic book, very readableJohn Schulman’s CS 294: Deep Reinforcement Learning (Berkeley)David Silver’s Reinforcement Learning course (UCL)YOU DID\\xa0IT!If you’ve made it this far, that is all the reward we could hope\\xa0for.We hope you enjoyed the series as an introduction to machine learning. We’ve compiled some of our favorite ML resources in the Appendix if you’re ready to see how deep this rabbit hole\\xa0goes.Please don’t hesitate to reach out with thoughts, questions, feedback, or your favorite\\xa0GIFs!Until next\\xa0time,Vishal and\\xa0SamerClosing thoughtsThere is a fundamental question that inspired this series, and we\\'d like to pose it to you as well. What is our objective function, as humans? How do we define the reward that we maximize in our real lives? Beyond base pleasure and pain, our definition of reward also tends to include messy things like right and wrong, fulfillment, love, spirituality, and purpose.There has been an intellectual field dedicated to the question of what our objective functions are or should be since ancient times, and it’s called moral philosophy. The central question of moral philosophy is: what ought we do? How should we live? Which actions are right or wrong? The answer is, quite clearly: it depends on your values.As we create more and more advanced AI, it will start to depart from the realm of toy problems like Atari games, where “reward” is cleanly defined by how many points are won in the game,  and exist more and more in the real world. Autonomous vehicles, for example, have to make decisions with a somewhat more complex definition of reward. At first, reward might be tied to something like “getting safely to the destination\". But if forced to choose between staying the course and hitting five pedestrians or swerving and hitting one, should the vehicle swerve? What if the one pedestrian is a child, or a gunman on the loose, or the next Einstein? How does that change the decision, and why? What if swerving also destroys a piece of valuable art? Suddenly we have a much more complex problem when we try to define the objective function, and the answers are not as simple.In this series, we explored why it’s difficult to specify explicitly to a computer what a cat looks like - if asked how we know ourselves, the answer is, most simply, “intuition” - but we’ve explored machine vision approaches to teaching the machine to learn this intuition by itself. Similarly, in the domain of machine morality, it might be difficult to specify exactly how to evaluate the rightness or wrongness of one action vs. another, but perhaps it is possible for a machine to learn these values in some way. This is called the values learning problem, and it may be one of the most important technical problems humans will ever have to solve.For more on this topic, see this synoptic post on the Risks of Artificial Intelligence. And as you go forth into the world of making machine smarter and smarter, we\\'d encourage you to keep in mind that AI progress is a double-edged sword, of particular keenness on both sides.Enter your email below if you’d like to stay up-to-date with future content\\xa0💌On Twitter? So are we. Feel free to keep in touch\\u200a—\\u200aVishal and Samer\\xa0🙌🏽If you found this series useful and would like to support the authors, we would appreciate any amount you\\'re able to contribute to one of the addresses below :)Bitcoin: 1Fei5xpXQuSazzkaYHMS6PJ92oZ8YDqLnmEthereum: 0x949696c9062a28f54b8166b9b1d29CB2a170D9BeBitcoin Cash: 1Fei5xpXQuSazzkaYHMS6PJ92oZ8YDqLnmPart 1: Why Machine Learning Matters ✅Part 2.1: Supervised Learning ✅Part 2.2: Supervised Learning II ✅Part 2.3: Supervised Learning III ✅Part 3: Unsupervised Learning ✅Part 4: Neural Networks & Deep Learning ✅Part 5: Reinforcement Learning ✅Appendix: The Best Machine Learning ResourcesThanks to Sachin Maini. Machine LearningArtificial IntelligenceDeep LearningReinforcement LearningTechOne clap, two clap, three clap, forty?By clapping more or less, you can signal to us which stories really stand out.Vishal MainiResearch comms @DeepMindAI. Previously @Upstart, @Yale, @TrueVenturesTEC.Machine Learning for HumansDemystifying artificial intelligence & machine learning. Discussions on safe and intentional application of AI for positive social impact.'}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mycorpus.get_nth(3622)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ids2articles(ids, corpus): \n",
    "    \"\"\"Returns article objects, given a list of ids\"\"\"\n",
    "    article_objects = []\n",
    "    for article_id in ids: \n",
    "        article_objects.append(corpus.get_nth(article_id))\n",
    "    return article_objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This takes a pretty long time. Can save time by using a db for sure. \n",
    "articles = ids2articles(top_ids, mycorpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://hackernoon.com/rational-agents-for-artificial-intelligence-caf94af2cec5\n",
      "Rational Agents for Artificial Intelligence – Hacker Noon\n",
      "0.31066835\n",
      "******************************\n",
      "https://hackernoon.com/mit-6-s094-deep-learning-for-self-driving-cars-2018-lecture-3-notes-deep-reinforcement-learning-fe9a8592e14a\n",
      "MIT 6.S094: Deep Learning for Self-Driving Cars 2018 Lecture 3 Notes: Deep Reinforcement Learning\n",
      "0.29828316\n",
      "******************************\n",
      "https://medium.com/nectec/reinforcement-learning-43ea03c2e00e\n",
      "Reinforcement Learning – NECTEC – Medium\n",
      "0.29372314\n",
      "******************************\n",
      "https://medium.com/machine-learning-for-humans/neural-networks-deep-learning-cdad8aeae49b\n",
      "Machine Learning for Humans, Part 4: Neural Networks & Deep Learning\n",
      "0.28961307\n",
      "******************************\n",
      "https://hackernoon.com/nathan-ai-newsletter-issue-21-part-2-2-e6e0e7ab3ce\n",
      "Nathan.ai newsletter Issue #21 — Part 2/2 – Hacker Noon\n",
      "0.25882074\n",
      "******************************\n"
     ]
    }
   ],
   "source": [
    "for article, score in zip(articles, top_scores): \n",
    "    print(article['url'])\n",
    "    print(article['title'])\n",
    "    print(score)\n",
    "    print('*'*30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommend With LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = LdaModel.load('./models/lda-sample10000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process User text \n",
    "lda_vec = lda_model[dictionary.doc2bow(user_text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use LDA for similarity index. Also takes forever to load in\n",
    "lda_index = similarities.Similarity(None, lda_model[loaded_corpus], len(dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.77258664, 0.8379322 , 0.53435254, ..., 0.29647186, 0.8981663 ,\n",
       "       0.7700697 ], dtype=float32)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_top_ids, lda_top_scores = get_top_n(5, lda_index, lda_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_articles = ids2articles(top_ids, mycorpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://hackernoon.com/rational-agents-for-artificial-intelligence-caf94af2cec5\n",
      "Rational Agents for Artificial Intelligence – Hacker Noon\n",
      "0.97150904\n",
      "******************************\n",
      "https://hackernoon.com/mit-6-s094-deep-learning-for-self-driving-cars-2018-lecture-3-notes-deep-reinforcement-learning-fe9a8592e14a\n",
      "MIT 6.S094: Deep Learning for Self-Driving Cars 2018 Lecture 3 Notes: Deep Reinforcement Learning\n",
      "0.960182\n",
      "******************************\n",
      "https://medium.com/nectec/reinforcement-learning-43ea03c2e00e\n",
      "Reinforcement Learning – NECTEC – Medium\n",
      "0.94829416\n",
      "******************************\n",
      "https://medium.com/machine-learning-for-humans/neural-networks-deep-learning-cdad8aeae49b\n",
      "Machine Learning for Humans, Part 4: Neural Networks & Deep Learning\n",
      "0.9453678\n",
      "******************************\n",
      "https://hackernoon.com/nathan-ai-newsletter-issue-21-part-2-2-e6e0e7ab3ce\n",
      "Nathan.ai newsletter Issue #21 — Part 2/2 – Hacker Noon\n",
      "0.9409206\n",
      "******************************\n"
     ]
    }
   ],
   "source": [
    "for article, score in zip(lda_articles, lda_top_scores): \n",
    "    print(article['url'])\n",
    "    print(article['title'])\n",
    "    print(score)\n",
    "    print('*'*30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the scorin metrics may be different, the LDA and TFIDF models both return the same top 5 articles for this dataset. \n",
    "This dataset contained 5000 articles sampled randomly from the original dataset of ~65000 medium articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test User Details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "email: doe@example.com\n",
    "password: asdf\n",
    "\n",
    "classes: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'code': 'INSY402', 'title': 'Internet Technologies'}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{'code': 'INSY404', 'title': \"Introduction to Object Oriented Design\"},\n",
    "{'code': 'COSC418', 'title': \"Introduction to Analog Computing\"},\n",
    "{'code': 'COSC408', 'title': \"Modelling and Simulations\"},\n",
    "{'code': 'INSY402', 'title': \"Internet Technologies\"}\n",
    "{'code': 'COSC423', 'title': \"Artificial Intelligence\"}\n",
    "{'code': 'COSC417', 'title': \"Database Design and Management\"}\n",
    "{'code': 'COSC430', 'title': \"Hands-on Java Training\"},\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
